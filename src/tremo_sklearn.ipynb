{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.tr import Turkish\n",
    "from  spacy.lang.tr.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#sentiment analysis with DBN (deep belief network)\n",
    "from dbn.tensorflow import SupervisedDBNClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>ValidatedEmotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>her yeni gün bir mutluluk</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gece kimsenin olmadığı sokaklardan geçerken ço...</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gerçekleşemeyen hayaller</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arkadaş kaybetmek beni üzüyor</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insanların çıkarcı olmalarından tiksiniyorum</td>\n",
       "      <td>Disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Entry ValidatedEmotion\n",
       "0                          her yeni gün bir mutluluk            Happy\n",
       "1  gece kimsenin olmadığı sokaklardan geçerken ço...             Fear\n",
       "2                           gerçekleşemeyen hayaller          Sadness\n",
       "3                      arkadaş kaybetmek beni üzüyor          Sadness\n",
       "4       insanların çıkarcı olmalarından tiksiniyorum          Disgust"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read tremo dataset\n",
    "tremopath = '/Users/pinarayaz/Jupyter/NLP/data/tremo.csv'\n",
    "tremo_df = pd.read_csv(tremopath)\n",
    "tremo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "punctuations = string.punctuation\n",
    "nlp = Turkish()\n",
    "#nlp = spacy.load(\"xx_ent_wiki_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords and punctuations\n",
    "stopwords = list(STOP_WORDS)\n",
    "punctuations = string.punctuation\n",
    "nlp = Turkish()\n",
    "#nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "#custom tokenizer for filtering\n",
    "def custom_tokenizer(sentence):\n",
    "    tokens = nlp(sentence)\n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "    return tokens\n",
    "\n",
    "#custom transformer using spacy \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "#basic function to clean the text \n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()\n",
    "\n",
    "#for transforming sparse matrix to dense\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        dense = X.todense()\n",
    "        rows = dense.shape[0]\n",
    "        cols = dense.shape[1]\n",
    "        to_pad = np.zeros((rows, rows-cols))\n",
    "        result = np.concatenate((dense, to_pad), axis=1)\n",
    "        print(result.shape)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer and classifier\n",
    "vectorizer = CountVectorizer(tokenizer = custom_tokenizer, ngram_range=(1,1))\n",
    "#vectorizer = TfidfVectorizer(tokenizer = custom_tokenizer)\n",
    "\n",
    "classifier = LinearSVC()\n",
    "#classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
    "                                         #learning_rate_rbm=0.05,\n",
    "                                         #learning_rate=0.1,\n",
    "                                         #n_epochs_rbm=1, #10\n",
    "                                         #n_iter_backprop=10, #100\n",
    "                                         #batch_size=32,\n",
    "                                         #activation_function='relu',\n",
    "                                         #dropout_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset\n",
    "X = tremo_df['Entry']\n",
    "ylabels = tremo_df['ValidatedEmotion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x1254d9ba8>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the pipeline to clean, tokenize, vectorize and classify\n",
    "pipeline = Pipeline([\n",
    "                    (\"cleaner\", predictors()), \n",
    "                    ('vectorizer', vectorizer),\n",
    "                    #('to_dense', DenseTransformer()), #for dbn model\n",
    "                    ('classifier', classifier)\n",
    "                    ])\n",
    "    \n",
    "#fit our data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.94\n",
      "Test Accuracy: 0.86\n",
      "Precision: 0.20\n",
      "Recall: 0.20\n",
      "F1 Score: 0.20\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "print(\"Train Accuracy: %.2f\" % pipeline.score(X_train, y_train))\n",
    "print(\"Test Accuracy: %.2f\" % pipeline.score(X_test, y_test))\n",
    "\n",
    "#calculate precision, recall, f1 score\n",
    "y_pred = pipeline.predict(y_test)\n",
    "print(\"Precision: %.2f\" % precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Recall: %.2f\" % recall_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"F1 Score: %.2f\" % f1_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy: 0.85 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "#perform cross validation\n",
    "scores = cross_val_score(pipeline, X, ylabels, cv=5)\n",
    "print(\"Cross Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
